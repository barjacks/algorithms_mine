<!DOCTYPE html>
<html>
  <head>
    <title>Algorithms - Lede Program</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="../slide.css"/>
  </head>
  <body>
    <textarea id="source">

layout:true

<p class="footer">
<span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Algorithms</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://www.datapolitan.com" property="cc:attributionName" rel="cc:attributionURL">Richard Dunks</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative-Commons-License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
</p>
---

class: center,middle

![img-center-50](../images/cl_logo.png)
- - -
#Algorithms: Trees, Trees, and More Trees
##Richard Dunks, Instructor
###Follow along: http://bit.ly/algo2016-class9

---

# Do Now
![img-right-50](../images/infogain2.png)
+ Write a function that takes in the three attributes of a Simpson character and outputs the gender classification
+ [Remember what we're splitting on](http://ledeprogram.github.io/algorithms/class8/#44)
+ Test the function with the Comic's attributes
+ Submit your code and give feedback on the Do Now in your PR

---

class:center,middle
# Check-in
![img-center-70](http://i.giphy.com/3eMEdQHc11MwU.gif)
## Did last Thursday help?

---

# Goals
+ Review decision trees
+ Implement a simple predictor function based on output from sci-kit learn
+ Review cross-validation
+ Implement a simple cross-validation function using a simple dataset

---

# Decision Tree redux
--

+ We have a computer calculate the best splits using a "greedy" model that strives for purity at each cut
--

+ Can use any measure of purity, including information gain/entropy or Gini Index
--

+ Results in a set of rules we can use to classify unseen instances

---

# Do Together
+ Using the tree we generated with the Iris data to code an Iris predictor
+ Use `if` statements to reproduce the decision points in the tree
+ Output the class level when you arrive at a leaf
+ We'll test our function on the following example

```
petal length = 4.5 cm
petal width = 1.51 cm
Should predict class 1 (iris versicolor)
```

---

#Advantages of decision trees
--

+ Easy to explain
--

+ Easy to implement
--

+ Work with features on different scales or mix of binary and continuous features
--

+ Can mix different trees together to get a better outcome (ensemble methods)

---

#Advantages of decision trees
+ Offer the opportunity to see the importance of features
--

```python
# running this on the iris decision tree
plt.plot(dt.feature_importances_,'o')
plt.xticks(range(iris.data.shape[1]),iris.feature_names, rotation=0)
plt.ylim(0,1)
```
--

![img-center-70](../images/iris_feature_importance.png)


---

class:center,middle
# 10 Min Break
![img-center-40](http://imgs.xkcd.com/comics/physicists.png)
Source: http://xkcd.com/793/

---

# Evaluation Methods
--

+ Holdout -> Reserve portion of data for training and testing 
--

+ Cross validation -> Partition data into k disjoint subsets and test

---

# Cross Validation
+ k-fold: train on k-1 partitions, test on the remaining one
--

![img-center-80](http://i.imgur.com/N9HZktu.png)
--

+ Leave-one-out (k=n): Train on all of the data except one instance and test on the left out instance

---

# Cross Validation
+ Example of 5-fold cross validation
![img-center-80](../images/cv1.png)

---

# Cross Validation
--

+ After testing with each fold of the data, a training model is created from 100% of the data
--

+ The performance of each fold is averaged together to generate an estimate of the model using all of the data
--

+ Using all of the data maximizes the available data

---

# Stratified Cross Validation
--

+ Each fold maintains the class percentages of the overall sample
--

+ For example: if 60% of the overall dataset is classified as female, then each of the folds will also be composed of samples that are approximately 60% male
--

+ This is useful in unbalanced classification exercises
--

+ For more information, see [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold) and the [scikit-learn Cross Validation Guide](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold)

---

class:center,middle
# Cross Validation in `sklearn`
## Open DecisionTree-CrossValidation.ipynb

---

class:center,middle
![img-center-100](http://i.giphy.com/Wow3QqPZLmgCI.gif)

---

# Let's see
--

+ In your groups, write pseudocode for a 5-fold CV function 
--

+ Take in input data (`np.array`) and model
--

+ Split into training and test across 5 iterations
--

+ Use the model functions to `fit`, `predict`, and `score`
--

+ Output an estimate of the model accuracy
--
<br>Your Groups<br>
`Hon Georgia Paolo Kate`<br>
`Monica Gianna DJ Jo`<br>
`Barney Mercy(B) Shuyao Radhika`<br>
`JJ Rebecca Shengying Sean`<br>

---

# Assigned Readings
+ "Random Forests" in Introduction to Machine Learning with Python
+ "Naive Bayes" in Doing Data Science

# Optional Readings
+ [scikit-learn documentation on cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html)
+ ["Random forests in Python"](http://blog.yhat.com/posts/random-forests-in-python.html)

---

# Assignment 1
+ Use the pseudocode you came up with in class to write your own 5-fold cross-validation function that splits the data set into 
+ Don't forget to shuffle the input before assigning to the splits
+ You can use the fit
+ Test the results with the sklearn `cross_val_score`
+ In your PR, discuss what challenges you had creating this function and if it helped you better understand cross validation

---

# Assignment 2
+ Using the readings, try and create a `RandomForestClassifier` for the iris dataset
+ Using a 25/75 training/test split, compare the results with the original decision tree model and describe the result to the best of your ability in your PR

---

# Assignment 3
+ Review the code in `homework/bill_classifier.ipynb'
+ Understand the steps in creating a text classifier
+ Comment in your PR on the utility of something like this for the work you've done or would like to do

---

# Questions going forward
--

+ How much did you do with text processing?
--

+ How much do you want to do with text processing?
---
class:center,middle
#Thank You!


    </textarea>
    <script src="../js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create(
        // {
        //   slideNumberFormat: ""}
        );
    </script>
  </body>
</html>