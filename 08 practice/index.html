<!DOCTYPE html>
<html>
  <head>
    <title>Algorithms - Lede Program</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="../slide.css"/>
  </head>
  <body>
    <textarea id="source">

layout:true

<p class="footer">
<span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Algorithms</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://www.datapolitan.com" property="cc:attributionName" rel="cc:attributionURL">Richard Dunks</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative-Commons-License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
</p>
---

class: center,middle

![img-center-50](../images/cl_logo.png)
- - -
# Algorithms
## Richard Dunks, Instructor
### Follow along: http://bit.ly/algo2016-class8

---

# Mid-Course Survey Results
--

+ 11 responses (out of 24 students, ~46%)
--

+ Thank you to those who filled it out
--

+ We have some issues to address and you'll notice some changes
--

+ So if you want to have your voice heard, please [fill it out](https://goo.gl/forms/U7rSq0o9uv1c9r4z1) if you haven't already

---

# Goals for today
+ Review 

---

# Quick Note
## Theses slides are based on the slides by 
+ Tan, Steinbach and Kumar (textbook authors)
+ Eamonn Koegh (UC Riverside)
+ Andrew Moore (CMU/Google)

---

# Decision Trees
--

+ We can have a computer calculate the best splits using a "greedy" model that strives for purity at each cut
--

+ Which of these cuts gives us a more pure cut between Class 0 (`C0`) and Class 1 (`C1`)?

![img-center-100](../images/gini1.png)

---

# How do we calculate purity?
--

+ Entropy and Information Gain
--

+ Gini Index

---

# Entropy
--

+ Measurement of order (or lack there of)
--

+ Increases as the set is more perfectly heterogenous and diminishes as the set is more homogenous (all positive or all negative)
--



![img-center-55](../images/entropy2.png)

---

# Entropy
+ Calculate as a function of the number of positives and negatives
--


![img-center-100](../images/entropy1.png)
### _p_ is number of positives and _n_ is number of negatives

---

# Information Gain
+ The reduction in entropy resulting from greater purity in a split
--


![img-center-100](../images/infogain1.png)

---

# Building a Decision Tree
![img-center-60](../images/dt1.png)
--

+ Select split based on highest information gain (reduction in entropy)

---

# Building a Decision Tree (The [ID3 Algorithm](https://en.wikipedia.org/wiki/ID3_algorithm))
--

+ Calculate the entropy of every attribute using the data set **_S_**
--

+ Split the set **_S_** into subsets using the attribute for which entropy is minimum (or, equivalently, information gain is maximum)
--

+ Make a decision tree node containing that attribute
--

+ Recurse on subsets using remaining attributes

---

# Example
![img-right-45](../images/infogain2.png)
--

![img-left-40](../images/infogain3.png)
--


![img-left-55](../images/infogain4.png)

---

![img-left-30](../images/infogain5.png)
--

```
Entropy(3F,2M)= -(3/5)log2(3/5) - (2/5)log2(2/5)
              =  0.9710
Entropy(1F,3M)= -(1/4)log2(1/4) - (3/4)log2(3/4)
              =  0.8113
```
--

```
Gain = 0.9911 – (4/9 * 0.8113 + 5/9 * 0.9710) 
     = 0.0911
```

--

- - -
![img-left-30](../images/infogain6.png)
--

```
Entropy(4F,1M)= -(4/5)log2(4/5) - (1/5)log2(1/5)
              =  0.7219
Entropy(0F,4M)= -(0/4)log2(0/4) - (4/4)log2(4/4)
              =  0
```
--

```
Gain = 0.9911 – (5/9 * 0.7219 + 4/9 * 0 ) 
     = 0.5900

```
<!-- &nbsp; -->
--

- - -
![img-left-30](../images/infogain7.png)
--

```
Entropy(1F,2M)= -(1/3)log2(1/3) - (2/3)log2(2/3)
              =  0.9183
Entropy(3F,3M)= -(3/6)log2(3/6) - (3/6)log2(3/6)
              =  1
```
--

```
Gain = 0.9911 – (6/9 * 1 + 3/9 * 0.9183 ) 
     = 0.0183
```
--

### Which is best?
--
 (Weight <= 160)
--
 .red[Information gain = 0.59]

---

#Gini Index
--

+ Similar to Information Gain in calculating purity
--

+ Calculate the purity based on probabilities
--

+ Slightly faster to calculate

---

# Calculating purity
--

+ No matter which measure we use, we keep doing this until we hit a stop condition
--


>### "Studies have shown that the choice of impurity measure has little effect on the performance of decision tree induction algorithms. This is because many impurity measures are quite consistent with each other [...]."
>### [Introduction to Data Mining](http://www-users.cs.umn.edu/~kumar/dmbook/index.php)
---

# Calculating purity
+ No matter which measure we use, we keep doing this until we hit a stop condition
+ You will never have to calculate purity
--

+ But you should understand the concept

---

#Stop Condition
--

+ All samples for a given node belong to the same class
--

+ There are no remaining attributes for further partitioning – majority voting is employed for classifying the leaf
--

+ There are no samples left (or only a predefined # of samples left)

---

# Overfitting
![img-center-90](../images/overfit1.png)

---

# Overfitting
![img-center-90](../images/overfit2.png)

---

# Pre-Pruning
--

+ Identify overfitting as it happens
--

+ Limit creation of new nodes
--

+ Hard to know in advance when you need to prune

---

# Post-pruning
--

+ Go back after the fact to trim nodes
--

+ More common method

---

# Evaluation Methods
--

+ Holdout -> Reserve portion of data for training and testing 
--

+ Cross validation -> Partition data into k disjoint subsets and test

---

# Cross Validation
+ k-fold: train on k-1 partitions, test on the remaining one
--

![img-center-80](http://i.imgur.com/N9HZktu.png)
--

+ Leave-one-out (k=n): Train on all of the data except one instance and test on the left out instance

---

# Cross Validation
+ Example of 5-fold cross validation
![img-center-80](../images/cv1.png)

---

# Cross Validation
--

+ After testing with each fold of the data, a training model is created from 100% of the data
--

+ The performance of each fold is averaged together to generate an estimate of the model using all of the data
--

+ Using all of the data maximizes the available data

---

class:center,middle
#10 Min Break
![img-center-100](http://imgs.xkcd.com/comics/herpetology.png)
Source: http://xkcd.com/867/

---

# Confusion
![img-center-80](http://i.giphy.com/uPIq9daczoGyI.gif)
--

## It seems you all have it

---

# Now you're going to teach yourselves
--

+ Working in groups, you're going to take each line of code in one of the examples we've used and come up with a presentation to the class and take questions
--

+ Your task is to explain every package and function that is used to the best of your ability
--

+ Use manuals, tutorials, and the raw source code to explain it for the class
--

+ Make sure everyone in your group understands as well

---

# Groups
+ SimpleLinearRegression_Sklearn.ipynb<br>`Hon Georgia Paolo Kate`
+ SimpleLinearRegression_Statsmodels.ipynb<br>`Monica Gianna DJ Jo`
+ RegressionExample.ipynb `Harsha June Mathias Sasha`
+ FeatureEngineering.ipynb<br>`Barney Mercy(B) Shuyao Radhika`
+ DecisionTrees.ipynb `Paul Shannon Mercy(E) Olaya`
+ DecisionTrees-Validation.ipynb<br>`JJ Rebecca Shengying Sean`

---
class:center,middle
# Thank You!


    </textarea>
    <script src="../js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create(
        // {
        //   slideNumberFormat: ""}
        );
    </script>
  </body>
</html>