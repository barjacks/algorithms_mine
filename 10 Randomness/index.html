<!DOCTYPE html>
<html>
  <head>
    <title>Algorithms - Lede Program</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="../slide.css"/>
  </head>
  <body>
    <textarea id="source">

layout:true

<p class="footer">
<span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Algorithms</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://www.datapolitan.com" property="cc:attributionName" rel="cc:attributionURL">Richard Dunks</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative-Commons-License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
</p>
---

class: center,middle

![img-center-50](../images/cl_logo.png)
- - -
#Algorithms: How I Learned to Embrace Randomness
##Richard Dunks, Instructor
###Follow along: http://bit.ly/algo2016-class10

---

# Do Now
+ Build a simple decision tree classifier
+ Use the data in the `winequality` table
+ There's an outline in `class10/donow/`
+ .red[Make a copy of the notebook]

---

# Goals
--

+ Review homework on creating your own cross validation function
--

+ Introduce Bayesian models

---

# Assignment 1
+ Use the pseudocode you came up with in class to write your own 5-fold cross-validation function that splits the data set into 5 equal-sized sets
+ Don't forget to shuffle the input before assigning to sets
+ You can use the `fit()`, `predict()`, and `score()` functions of your model in your functions
+ Test the results with the sklearn `cross_val_score`
+ In your PR, discuss what challenges you had creating this function and if it helped you better understand cross validation


---

# Assignment 2 (Class 9)
+ Using the readings, try and create a `RandomForestClassifier` for the iris dataset
+ Using a 75/25 training/test split, compare the results with the original decision tree model and describe the result to the best of your ability in your PR

---

# Assignment 3 (Class 9)
+ Review the code in `homework/bill_classifier.ipynb`
+ Understand the steps in creating a text classifier
+ Comment in your PR on the utility of something like this for the work you've done or would like to do
---

class:center,middle
# 10 Min Break
![img-center-70](http://imgs.xkcd.com/comics/im_so_random.png)
Source: http://xkcd.com/1210/

---

# Assigned Readings (class 9)
+ "Random Forests" in Introduction to Machine Learning with Python
+ "Naive Bayes" in Doing Data Science

# Optional Readings
+ [scikit-learn documentation on cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html)
+ ["Random forests in Python"](http://blog.yhat.com/posts/random-forests-in-python.html)

---

# Theses slides are based on the slides by 
+ Tan, Steinbach and Kumar (textbook authors)
+ Eamonn Koegh (UC Riverside)
+ Andrew Moore (CMU/Google)

---

class:center,middle
# Conditional Probability and Bayes Theorem
## What was your take away from the reading?
## "Naive Bayes" in Doing Data Science

---

#Event Space in Probability
![img-center-100](../images/cp1.png)

---

# Conditional Probability
![img-right-50](../images/cp1a.png)

+ `H` is having a headache
+ `F` is coming down with the flu
--

+ P(H) = 1/10
--

+ P(F) = 1/40
--

+ P(H|F) = 1/2
--

## “Headaches are rare and flu is rarer, but if you’re coming down with the flu there’s a 50-50 chance you’ll have a headache.”

---

# Conditional Probability
--

+ P(A|B) = Fraction of worlds in which B is true that also have A true
--

+ Read it as "The probability of A given B"
--


![equation](http://bit.ly/2b8ywG3)

--

+ The **prior probability** is the probability assuming no specific information (like P(H))
--

+ The **posterior probability** is the probability given that we know something (like P(H|F))

---

# Probablistic Inference
![img-right-50](../images/cp1a.png)

+ `H` is having a headache
+ `F` is coming down with the flu
+ P(H) = 1/10
+ P(F) = 1/40
+ P(H|F) = 1/2

### One day you wake up with a headache. You think: “Drat! 50% of flus are associated with headaches so I must have a 50-50 chance of coming down with flu”
--
 .blue[Is this reasoning valid?]

---

# Probablistic Inference
![img-right-50](../images/cp1a.png)

+ `H` is having a headache
+ `F` is coming down with the flu
+ P(H) = 1/10
+ P(F) = 1/40
+ P(H|F) = 1/2


![img-center-80](../images/pfh1.png)
--

![img-center-65](../images/pfh2.png)

---

# Another Example
--

+ A doctor knows that meningitis causes stiff neck 50% of the time
--

+ Prior probability of any patient having meningitis is 1/50,000
--

+ Prior probability of any patient having stiff neck is 1/20
--


## If a patient has stiff neck, what’s the probability he/she has meningitis?
--

![img-center-90](../images/nbex1.png)

---

# Naive Bayes Classifier
--

+ A model that uses the probability an instance belongs to a class based on the known relationship between features
--

+ Predict a class (C) based on the probabilities, knowing the relationship of features (F<sub>1</sub> and F<sub>2</sub>)
--

+ We "naively" assume them to be independent
--

![equation](http://www.sciweavers.org/tex2img.php?eq=%24%24%20P%28C%20%5Cmid%20F_1%2CF_2%29%20%3D%20%5Cfrac%7BP%28F_1%2CF_2%20%5Cmid%20C%29%20%5C%2C%20P%28C%29%7D%7BP%28F_1%2CF_2%29%7D%20%24%24&bc=White&fc=Black&im=jpg&fs=48&ff=arev&edit=0)

---

# Naive Bayes Classifier
![equation](http://www.sciweavers.org/tex2img.php?eq=%24%24%20P%28C%20%5Cmid%20F_1%2CF_2%29%20%3D%20%5Cfrac%7BP%28F_1%2CF_2%20%5Cmid%20C%29%20%5C%2C%20P%28C%29%7D%7BP%28F_1%2CF_2%29%7D%20%24%24&bc=White&fc=Black&im=jpg&fs=48&ff=arev&edit=0)
+ P(C) is the prior probability of class C without knowing about the data
--

+ This quantity can be obtained by simply calculating the fraction of all training data instances belonging to that particular class
---

# Naive Bayes Classifier
![equation](http://www.sciweavers.org/tex2img.php?eq=%24%24%20P%28C%20%5Cmid%20F_1%2CF_2%29%20%3D%20%5Cfrac%7BP%28F_1%2CF_2%20%5Cmid%20C%29%20%5C%2C%20P%28C%29%7D%7BP%28F_1%2CF_2%29%7D%20%24%24&bc=White&fc=Black&im=jpg&fs=48&ff=arev&edit=0)
+ P(F<sub>1</sub>, F<sub>2</sub>) is the evidence, or the probability of features F<sub>1</sub> and F<sub>2</sub>
--

+ This can be retrieved by calculating the fraction of all training data instances having that particular feature value

---

# Naive Bayes Classifier
![equation](http://www.sciweavers.org/tex2img.php?eq=%24%24%20P%28C%20%5Cmid%20F_1%2CF_2%29%20%3D%20%5Cfrac%7BP%28F_1%2CF_2%20%5Cmid%20C%29%20%5C%2C%20P%28C%29%7D%7BP%28F_1%2CF_2%29%7D%20%24%24&bc=White&fc=Black&im=jpg&fs=48&ff=arev&edit=0)
+ The tricky part is the calculation of the likelihood P(F<sub>1</sub>, F<sub>2</sub> | C) 
--

+ It is the value describing how likely it is to see feature values F<sub>1</sub> and F<sub>2</sub> if we know that the class of the data instance is C

---

# Naive Bayes Classifier
![equation](http://www.sciweavers.org/tex2img.php?eq=%24%24%20P%28C%20%5Cmid%20F_1%2CF_2%29%20%3D%20%5Cfrac%7BP%28F_1%2CF_2%20%5Cmid%20C%29%20%5C%2C%20P%28C%29%7D%7BP%28F_1%2CF_2%29%7D%20%24%24&bc=White&fc=Black&im=jpg&fs=48&ff=arev&edit=0)

---

# Naive Bayes Classifier
--

+ NB Classifiers learn parameters by looking at each feature individually, and collect simple per-class statistics from each feature
--

+ To make a prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted

---

# Naive Bayes in `sklearn`
--

+ `GaussianNB` can be applied to any continuous data
--

+ `BernoulliNB` assumes binary data and 
--

+ `MultinomialNB` assumes count data (that is each feature represents an integer count of something, like how often a word appears in a sentence)
--

+ `BernoulliNB` and `MultinomialNB` are mostly used in text data classification

---

# Advantages of Naive Bayes
--

+ Fast to train and to predict
--

+ Training procedure is relatively easy to understand
--

+ The models work very well with high-dimensional sparse data
--

+ They are relatively robust to the parameters
--

+ Naive Bayes models are great baseline models, and 
--

+ They are often used on very large datasets, where training even a linear model might take too long


---

#Readings
+ ["Clustering with k-Means in Python"](https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/)
+ ["Clustering with k-Means is not a free lunch"](http://varianceexplained.org/r/kmeans-free-lunch/)
+ Doing Data Science, pg 71-86

---

# Assignment 1
+ TBD

---

class:center,middle
#Thank You!


    </textarea>
    <script src="../js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create(
        // {
        //   slideNumberFormat: ""}
        );
    </script>
  </body>
</html>